---
output: 
  stevetemplates::beamer:
    latex_engine: xelatex # use xelatex here instead! I recommend it, but this is minimal reprex
    dev: cairo_pdf # I typically comment this out  if latex_engine: pdflatex
    slide_level: 3 # I prefer this, but I won't force it on you.
    theme: metropolis
title: "Bivariate OLS"
subtitle: EH6105 -- Quantitative Methods
author: Steven V. Miller
institute: Department of Economic History and International Relations
titlegraphic: "`r paste0(Sys.getenv('HOME'), '/Dropbox/stockholm/su-logotyp.png')`"
titlegraphshift: "\\vspace{6cm}"
make149: true
mainfont: "Open Sans Light"
titlefont: "Titillium Web"
fontsize: 10pt
header-includes:
- \usepackage{dcolumn}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
---



```{r setup, include=FALSE, cache=F, message=F, warning=F, results="hide"}
knitr::opts_chunk$set(cache=TRUE, warning=F, message=F, echo=TRUE, fig.width = 14, fig.height = 8.5)
knitr::opts_chunk$set(fig.path='figs/')
knitr::opts_chunk$set(cache.path='cache/',
                      collapse = TRUE, comment = "#>")

knitr::opts_chunk$set(
                  fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      }
                  )
```


```{r loadstuff, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
options(knitr.kable.NA = '')
library(tidyverse)
library(stevemisc)
library(stevedata)
library(stevethemes)

eurostat_codes %>% 
  filter(cat == "EU" | iso2c == "UK") %>% 
  pull(iso2c) -> isocodes_we_want

wbd_example %>% na.omit %>%
  filter(year == 2020)  %>% 
  filter(iso2c %in% isocodes_we_want) -> Data

options("modelsummary_format_numeric_latex" = "plain")
options(knitr.kable.NA = '')
```

```{r loaddata, cache=F, eval=T, echo=F, message=F, error=F, warning=F}

```


# Introduction
### Goal for Today

*Use correlation and linear regression to describe the relationship between two continuous variables.*

### Building Toward Normal Social Science

Everything we have done is building toward normal quantitative research.

- We have concepts of interest, operationalized to variables.
- We observe central tendencies and variation in our variables.
- We believe there is cause and effect.
	- Though, importantly, we need to make controlled comparisons.
- We learned about random sampling and hypothesis testing.

If our sample statistic is more than 1.96 standard errors from a proposed population parameter, we suggest a population parameter is highly unlikely given what we got.

- This is admittedly an indirect answer to the question you're not asking, but this is what we're doing.

### What We Will Be Doing Today

We'll go over the following two topics.

1. **Correlation analysis**
2. **Regression analysis**


# Correlation
### Correlation

*Question*: does a country's life expectancy vary by its human capital?

<!-- - Education: % of state with high school diploma. (CPS estimates for 2015) -->
<!-- - Turnout: voter turnout for highest office (i.e. president) in 2016 general election. -->

- Human capital (index): how well today can citizen expect to achieve full health and achieve her formal education potential. [0:1]
- Life expectancy: average life expectancy form men and women (in years)
- *Data subset to 2020 for states in the EU (incl. the UK)*

We get a preliminary judgment using a **scatterplot**.

<!-- - But first: let's look at our data a bit. -->

<!-- ### Least Educated States -->

<!-- ```{r} -->
<!-- election_turnout %>% select(state, perhsed) %>%  -->
<!--   top_n(-5, perhsed) %>% arrange(perhsed) -->
<!-- ``` -->

<!-- ### Be Mindful of Your Education Indicator... -->

<!-- ```{r} -->
<!-- election_turnout %>% select(state, percoled) %>%  -->
<!--   top_n(-5, percoled) %>% arrange(percoled) -->
<!-- ``` -->

<!-- ### What About the Most Educated? -->

<!-- ```{r} -->
<!-- election_turnout %>% select(state, perhsed) %>%  -->
<!--   top_n(5, perhsed) %>% arrange(-perhsed) -->
<!-- ``` -->

<!-- ### On Voter Turnout in 2016... -->

<!-- ```{r} -->
<!-- election_turnout %>% select(state, turnoutho) %>%  -->
<!--   top_n(5, turnoutho) %>% arrange(-turnoutho) -->
<!-- ``` -->

<!-- ### Lowest Turnout States -->

<!-- ```{r} -->
<!-- election_turnout %>% select(state, turnoutho) %>%  -->
<!--   top_n(-5, turnoutho) %>% arrange(turnoutho) -->
<!-- ``` -->

###

```{r scatter1, echo=F, eval=T, warning=F}

ggplot(Data, aes(hci, lifeexp)) + 
  geom_point(size=I(2)) + 
  theme_steve() +
  #xlim(80,95) +
  #scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
  labs(title = "A Scatterplot of Human Capital and Life Expectancy in 2020",
       subtitle = "The data are scattered in a formal consistent/positive way.",
       x = "Human Capital Index",
       y = "Life Expectancy in Years for Men and Women",
       caption = "Data: ?wbd_example in {stevedata}, by way of World Bank.")

```


### Correlation

This relationship looks easy enough: positive.

- The relationship is not perfect, but it looks fairly "strong".

How strong? **Pearson's correlation coefficient** (or **Pearson's *r***) will tell us.

### Pearson's *r*

$$
    \Sigma\frac{(\frac{x_i - \overline{x}}{s_x})(\frac{y_i - \overline{y}}{s_y})}{n - 1}
$$

...where:

- $x_i$, $y_i$ = individual observations of *x* or *y*, respectively.
- $\overline{x}$, $\overline{y}$ = sample means of *x* and *y*, respectively.
- $s_x$, $s_y$ = sample standard deviations of *x* and *y*, respectively.
- *n* = number of observations in the sample.

<!-- ### Properties of Pearsons *r* -->

<!-- 1. Pearson's *r* is symmetrical. -->
<!-- 2. Pearson's *r* is bound between -1 and 1. -->
<!-- 3. Pearson's *r* is standardized. -->

###

```{r scatterz, echo=F, eval=T, warning=F, error=F, message=F}
library(ggrepel)
Data %>%
  r1sd_at(c("lifeexp", "hci")) %>%
  ggplot(.,aes(s_hci, s_lifeexp)) +
  geom_point(size=I(2)) + 
  theme_steve() +
  # xlim(-2.2,2) + ylim(-3.3,3.3) +
  geom_vline(xintercept=0, linetype="dashed") +
  geom_hline(yintercept=0, linetype="dashed") +
  scale_y_continuous(breaks = c(-3,-2,-1,0,1,2,3), limits=c(-3.1,3)) +
  geom_label_repel(aes(label=ifelse(s_hci > 0 & s_lifeexp < 0 ,as.character(country),'')), family="Open Sans") +
  geom_label_repel(aes(label=ifelse(s_hci < 0 & s_lifeexp > 0 ,as.character(country),'')), family="Open Sans") +
    labs(title = "A Scatterplot of Human Capital and Life Expectancy in 2020",
        subtitle = "Observations in the negative correlation quadrants are highlighted for emphasis.",
       x = "Human Capital Index  (Standardized)",
       y = "Life Expectancy in Years for Men and Women (Standardized)",
       caption = "Data: ?wbd_example in {stevedata}, by way of World Bank.")

```


### Education and Turnout (z-scores)

- Cases in upper-right quadrant are above the mean in both *x* and *y*.
- Cases in lower-left quadrant are below the mean in both *x* and *y*.
- Upper-left and lower-right quadrants are negative-correlation quadrants.

All told, our Pearson's *r* is 16.91/24, or about .70

- We would informally call this a fairly strong positive relationship.

<!-- ### ...or in R -->

<!-- \scriptsize -->
```{r orinr, echo=F, eval=F}

Data %>%
  r1sd_at(c("lifeexp", "hci")) -> Data

with(Data, sum(s_lifeexp*s_hci)/(length(country)-1))

Data %>%
  summarize(cor = cor(hci,lifeexp)) %>%
  pull()

```
\normalsize

# Linear Regression
## Demystifying Regression
### Linear Regression

Correlation has a lot of nice properties.

- It's another "first step" analytical tool.
- Useful for detecting **multicollinearity**.
	- This is when two independent variables correlate so highly that no partial effect for either can be summarized.

However, it's neutral on what is *x* and what is *y*.

- It won't communicate cause and effect.

Fortunately, regression does that for us.

### Demystifying Regression

Does this look familiar?

$$
y = mx + b
$$

### Demystifying Regression

That was the slope-intercept equation.

- *b* is the intercept: the observed *y* when *x* = 0.
- *m* is the familiar "rise over run", measuring the amount of change in *y* for a unit change in *x*.

### Demystifying Regression

The slope-intercept equation is, in essence, the representation of a regression line.

- However, statisticians prefer a different rendering of the same concept measuring linear change.

$$
y = a + b(x)
$$

The *b* is the **regression coefficient** that communicates the change in *y* for each unit change in *x*.

## A Simple Example
### A Simple Example

Suppose I want to explain your test score (*y*) by reference to how many hours you studied for it (*x*).

| *Hours (x)* | *Score (y)* |
|:------------|:-----------:|
| 0 | 55 |
| 1 | 61 |
| 2 | 67 |
| 3 | 73 |
| 4 | 79 | 
| 5 | 85 | 
| 6 | 91 |
| 7 | 97 |

Table: Hours Spent Studying and Exam Score

<!-- ### 


![Hours Spent Studying and Test Score](tab81.pdf) -->

### A Simple Example

In this eight-student class, the student who studied 0 hours got a 55.

- The student who studied 1 hour got a 61.
- The student who studied 2 hours got a 67.
- ...and so on...

Each hour studied corresponds with a six-unit change in test score. Alternatively:

$$
y = a + b(x) = \textrm{Test Score} = 55 + 6(x)
$$

Notice that our *y*-intercept is meaningful.

### A Slightly Less Simple Example

However, real data are never that simple. Let's complicate it a bit.

\scriptsize

| *Hours (x)* | *Score (y)* | *Estimated Score ($\hat{y}$)* |
|:------------|:-----------:|:---------------------:|
| 0 | 53 | 55 |
| 0 | 57 | |
| 1 | 59 | 61 |
| 1 | 63 | |
| 2 | 65 | 67 |
| 2 | 69 | |
| 3 | 71 | 73 |
| 3 | 75 | |
| 4 | 77 | 79 |
| 4 | 81 | |
| 5 | 83 | 85 |
| 5 | 87 | |
| 6 | 89 | 91 |
| 6 | 93 | |
| 7 | 95 | 97 |
| 7 | 99 | |

Table: Hours Spent Studying, Exam Score, and Estimated Score 

\normalsize

<!-- ###

![Hours Spent Studying and Test Score](tab82.pdf)
-->

### A Slightly Less Simple Example

Complicating it a bit doesn't change the regression line.

- Notice that regression averages over differences.
- An additional hour studied, *on average*, corresponds with a six-unit increase in the exam score.
- We have observed data points (*y*) and our estimates ($\hat{y}$, or *y*-hat).

## Getting a Regression Coefficient
### Our Full Regression Line

Thus, we get this form of the regression line.

$$
\hat{y} = \hat{a} + \hat{b}(x) + e
$$

...where:

- $\hat{y}$, $\hat{a}$ and $\hat{b}$ are estimates of *y*, *a*, and *b* over the data.
- *e* is the error term.
	- It contains random sampling error, prediction error, and predictors not included in the model.

### Getting a Regression Coefficient

How do we get a regression coefficient for more complicated data?

- Start with the **prediction error**, formally: $y_i - \hat{y}$.
- Square them. In other words: $(y_i - \hat{y})^2$
	- If you didn't, the sum of prediction errors would equal zero.

The regression coefficient that emerges minimizes the sum of squared differences ($(y_i - \hat{y})^2$).

- Put another way: "ordinary least squares" (OLS) regression.

The next figure offers a representation of this for our example.

<!-- ### Regression: Education and Turnout

![Scatterplot: Education and Turnout (A Regression)](fig83.pdf) -->

### 

```{r scatterline, echo=F, eval=T, warning=F}

ggplot(Data, aes(hci, lifeexp)) + 
  geom_point(size=I(2)) + 
  theme_steve() +
  geom_smooth(method=lm, se=FALSE) +
  #xlim(80,95) +
  #scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
  labs(title = "A Scatterplot of Human Capital and Life Expectancy in 2020",
       subtitle = "The line that minimizes the sum of squared prediction errors is drawn through these points.",
       x = "Human Capital Index",
       y = "Life Expectancy in Years for Men and Women",
       caption = "Data: ?wbd_example in {stevedata}, by way of World Bank.")
# 
# ggplot(election_turnout, aes(perhsed, turnoutho)) + 
#   geom_point(size=I(2)) + theme_steve_web() +
#   xlim(80,95) +
#   scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
#   geom_smooth(method=lm, se=FALSE) +
#   labs(title = "Education and Turnout in the 2016 General Election",
#        x = "Percentage of Residents 25-years-and-older with at Least a High School Diploma",
#        y = "Percentage Turnout of VEP for Highest Office",
#        subtitle = "The line that minimizes the sum of squared prediction errors is drawn through these points.")

```

### How You'd Get What You Want in R

\scriptsize

```{r basiclm}
summary(M1 <- lm(lifeexp ~ hci, data=Data))
```

\normalsize


### On the Output You See

The important stuff:

- "Estimate": y-intercept, and regression coefficients (i.e. "rise over run")
- Standard errors: an estimate of variability around the estimate (coefficient).
- Test statistic stuff ($t$-statistic, $p$-value): the stuff you'll use for inference.
- $R^2$s: measures of how well the model fit the data.

The less important stuff:

- $F$-statistic: "overall significance" of the model.
- Residual standard error: standard error of the residuals
    - Used for calculating standard errors, in combination with the var-cov matrix (which you don't see).
- Distribution of residuals (at the top): provides a summary of the range of residuals.

### Standard Error of Regression Coefficient

Each parameter in the regression model comes with a "standard error."

- These estimate how precisely the model estimates the coefficient's unknown value.

This has a convoluted estimation procedure.

- Namely: you need the diagonal of the square root of the variance-covariance matrix.
- This requires matrix algebra, and I hate matrix algebra. :P

It's standard output in a regression formula object in R, though.


<!-- ### If You're Curious... -->

<!-- \scriptsize -->

```{r extractses, echo=FALSE, eval=F}
X <- model.matrix(M1) # Intercept + perhsed

# Residual sum of squares
sigma2 <- sum((election_turnout$turnoutho - fitted(M1))^2) / (nrow(X) - ncol(X))

sqrt(sigma2) # residual standard error
sqrt(diag(solve(crossprod(X))) * sigma2) 
```

\normalsize

### 

```{r scatterlinese, echo=F, eval=T, warning=F}

ggplot(Data, aes(hci, lifeexp)) + 
  geom_point(size=I(2)) + 
  theme_steve() +
  geom_smooth(method=lm, se=TRUE) +
  #xlim(80,95) +
  #scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
  labs(title = "A Scatterplot of Human Capital and Life Expectancy in 2020",
       subtitle = "The line that minimizes the sum of squared prediction errors is drawn through these points.",
       x = "Human Capital Index",
       y = "Life Expectancy in Years for Men and Women",
       caption = "Data: ?wbd_example in {stevedata}, by way of World Bank.")

# ggplot(election_turnout, aes(perhsed, turnoutho)) + 
#   geom_point(size=I(2)) + theme_steve() +
#   xlim(80,95) +
#   scale_y_continuous(breaks = c(40,45,50,55,60,65,70,75), limits=c(40,75)) +
#   geom_smooth(method=lm, se=TRUE) +
#     labs(title = "Education and Turnout in the 2016 General Election",
#        x = "Percentage of Residents 25-years-and-older with at Least a High School Diploma",
#        y = "Percentage Turnout of VEP for Highest Office",
#        subtitle = "The line that minimizes the sum of squared prediction errors is drawn through these points.")

```

### Regression: Education and Turnout

This would be our regression line:

$$
\hat{y} = 51.102 + 38.941(x)
$$

How to interpret this:

- The country for which human capital is 0 would have an average life expectancy of 51.102.
    - This would never be observed, but it's at least a plausible quantity.
- A one-unit increase in human capital corresponds with an estimate increase in life expectancy of 38.941 years.
    - Given this variable's scale, this is incidentally a min-max effect.

<!-- - The state in which no one graduated from high school would have a voter turnout of -32.30%. -->
<!-- 	- This is obvious nonsense, which is why you'll want to learn about variable transformations as you progress. -->

## Inference in Regression
### Inference in Regression

What do we say about that *b*-hat ($\hat{b}$ = 38.941?)

- If we took another "sample", would we observe something drastically different?
- How would we know?

### Inference in Regression

You've done this before. Remember our last lectures? And z-scores?

$$
Z = \frac{\overline{x} - \mu}{s.e.}
$$

### Inference in Regression

We do the same thing, but with a Student's *t*-distribution.

$$
t = \frac{\hat{b} - \beta}{s.e.}
$$

$\hat{b}$ is our regression coefficient. What is our $\beta$?

### Inference in Regression

$\beta$ is actually zero!

- We are testing whether our regression coefficient is an artifact of the "sampling process".
- We're testing a competing hypothesis that there is no relationship between *x* and *y*.
    - This is the "null hypothesis" you'll read about in your travels.

### Inference in Regression

This makes things a lot simpler.

$$
t = \frac{\hat{b}}{s.e.}
$$

### Inference in Regression

In our example, this turns out nicely.

$$
t = \frac{38.941}{8.177} = 4.762
$$

Our regression coefficient is more than four standard errors from zero .

- The probability of observing it if $\beta$ were really zero is 0.0000842

We judge our regression coefficient to be "statistically significant."

- This is a fancy (and misleading) way of saying "it's highly unlikely to be 0."

<!-- ### Alternatively, in R... -->

<!-- \scriptsize -->

```{r, echo=F, eval=F}
# lm() in R is doing this for you, but let's do it ourselves...
# Be mindful there is some rounding for presentation.
broom::tidy(M1)
# Let's just get the variable we want.
broom::tidy(M1) %>% slice(2) -> info_we_want

# divide the coefficient...
pull(info_we_want[1,2])/
  # ...over the standard error and...
  pull(info_we_want[1,3]) -> t_stat # ...assign to object

t_stat
# two-tail test time
2*pt(t_stat, 46, lower.tail=FALSE) # hi mom!
```

\normalsize

# Conclusion
### Conclusion

Hopefully, this lecture demystified regression.

- It builds on everything discussed to this point.
- The same process of inference from sample to population is used.
- Really nothing to it but to do it, I 'spose.

We’re going to add a fair bit on top of this next.

- If you understand this, everything else to follow is basically window dressing.
